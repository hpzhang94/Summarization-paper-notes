1. Exploring Domain Shift in Extractive Text Summarization:
  -first work that introduces domain shift to text summarization, treat news source as domain in MULTI-SUM dataset. 
  -Four strategies tried: joint training, pre-training, meta-learning, domain-aware model
  -very detailed and abundant experiments and analysis
  
  
2. Recurrent Neural Network for Text Classification with Multi-Task Learning
   -Early paper on RNN multi-task learning
   -3 models: Uniform-layer, Coupled-layer, Shared-layer models
   
3. Neural Summarization by Extracting Sentences and Words
   -first data-driven encoder-decoder structure
    
    
4. Searching for Effective Neural Extractive Summarization: What Works and What’s Next
    -Architecturally speaking, models with autoregressive decoder are prone to achieving better performance
    -The success of extractive summarization system on the CNN/DailyMail corpus heavily relies on the ability to learn positional information of the sentence.
    -Unsupervised transferable knowledge performs better
    -Sentence encoder doensn't matter too much
    -However, when we feed the entire article to BERT to obtain token representations and get the sentence representation through mean pooling, model performance soared to 42.3 R-1 score.
    -https://github.com/maszhongming/Effective_Extractive_Summarization

5. SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents
    -sequence label probelm formulation
    -content, salience, novelty, abs/rel position to classify
    -first to convert the abstractive summaries to extractive labels.

6. From Standard Summarization to New Tasks and Beyond: Summarization with Manifold Information
    -(1) Stream document; (2) Timeline document; (3) Extreme long document; (4) Dialog; (5) Query-based document; (6) Incorporating reader comment; (7) Template    based; (8)Multi-media;
    -Extreme Long Document Summarization arix and pubmed SOTA： Wen Xiao and Giuseppe Carenini. Extractive summarization of long documents by combining global and local context. In EMNLP, 2019.
    
7. Improved Code Summarization via a Graph Neural Network
    -can borrow the part of how to choose GNN, 
    
8.  A Comprehensive Survey on Graph Neural Networks
    -GNN survey, can be used to choose GNN and readout function
    
9. ROUGE: A Package for Automatic Evaluation of Summaries
    -ROUGE score paper and package
    -current research mostly use ROUGE 1/2/L F1 as evaluation
    
*10. Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization
    -centroid of all sentence vectors, then use similarity
    -similar to semantic matching
    -maybe think a way to combine with centrality unsupervised method
    
 *11. Unsupervised Aspect-Based Multi-Document Abstractive Summarization
     -cluster review senteces into aspect and then aggregate each cluster 
     -could try this idea: graph clustering + graph pooling
     
     
 12 Self-Attention Graph Pooling
     - a more complex and efficient pooling method compared to avg and max. 
     - readout function choice: max+ mean
     
 *13 Use graph to align documents
    - some useful sota paper: Exploiting Sentence Order in Document Alignment, 
    Multilevel Text Alignment with Cross-Document Attention, 
    Vecalign: Improved Sentence Alignment in Linear Time and Space
    Exploiting Sentence Similarities for Better Alignments
 
 14. Opinion summarziation:
      -Unsupervised Opinion Summarization with Noising and Denoising: self-supervised learning with  synthetic dataset and different noise
      -MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization: autoencoders for abstractive summarizaiton
      -Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised
 15. Constrastive learning in Summarization:
      -Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning
      -DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization
      -Contrastive Attention Mechanism for Abstractive Sentenc Summarization
      
 16. SUMMPIP: UNSUPERVISED MULTI-DOCUMENT SUMMARIZATION WITH SENTENCE GRAPH COMPRESSION
      - cluster and compress
      -compression uses keyphrase and shortest path
  
  17. ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks
      - scientic paper summarization with citation
      -dataset: CL-SciSumm, GCN Cited text spans
  18. TALKSUMM: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks
      -dataset
      
  19. LongSum, Laysum: two competition in EMNLP 2020 workshop.
  
  20. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents
      -provide dataset arix, pubmed
    
  21. A Neural Citation Count Prediction Model based on Peer Review Text
      -review text for academic paper
      -can think about with summarizaiton
      
  22. MLSUM: The Multilingual Summarization Corpus
      -a new mulitlingul corpus
      -can try something new in multilingual cross language
      
  23. A Preliminary Exploration of GANs for Keyphrase Generation
      -key phrase with GAN
      
  24.25 Intrinsic Evaluation of Summarization Datasets
        Earlier isn’t always better: Subaspect analysis on corpus and system biases in summarization
          evaluating the summarization datasets
 
  
