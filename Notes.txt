1. Exploring Domain Shift in Extractive Text Summarization:
  -first work that introduces domain shift to text summarization, treat news source as domain in MULTI-SUM dataset. 
  -Four strategies tried: joint training, pre-training, meta-learning, domain-aware model
  -very detailed and abundant experiments and analysis
  
  
2. Recurrent Neural Network for Text Classification with Multi-Task Learning
   -Early paper on RNN multi-task learning
   -3 models: Uniform-layer, Coupled-layer, Shared-layer models
   
3. Neural Summarization by Extracting Sentences and Words
   -first data-driven encoder-decoder structure
    
    
4. Searching for Effective Neural Extractive Summarization: What Works and What’s Next
    -Architecturally speaking, models with autoregressive decoder are prone to achieving better performance
    -The success of extractive summarization system on the CNN/DailyMail corpus heavily relies on the ability to learn positional information of the sentence.
    -Unsupervised transferable knowledge performs better
    -Sentence encoder doensn't matter too much
    -However, when we feed the entire article to BERT to obtain token representations and get the sentence representation through mean pooling, model performance soared to 42.3 R-1 score.
    -https://github.com/maszhongming/Effective_Extractive_Summarization

5. SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents
    -sequence label probelm formulation
    -content, salience, novelty, abs/rel position to classify
    -first to convert the abstractive summaries to extractive labels.

6. From Standard Summarization to New Tasks and Beyond: Summarization with Manifold Information
    -(1) Stream document; (2) Timeline document; (3) Extreme long document; (4) Dialog; (5) Query-based document; (6) Incorporating reader comment; (7) Template    based; (8)Multi-media;
    -Extreme Long Document Summarization arix and pubmed SOTA： Wen Xiao and Giuseppe Carenini. Extractive summarization of long documents by combining global and local context. In EMNLP, 2019.
    
7. Improved Code Summarization via a Graph Neural Network
    -can borrow the part of how to choose GNN, 
    
8.  A Comprehensive Survey on Graph Neural Networks
    -GNN survey, can be used to choose GNN and readout function
    
9. ROUGE: A Package for Automatic Evaluation of Summaries
    -ROUGE score paper and package
    -current research mostly use ROUGE 1/2/L F1 as evaluation
    
